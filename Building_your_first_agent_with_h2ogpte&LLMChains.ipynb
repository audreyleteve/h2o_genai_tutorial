{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6551bb72-4db4-4dbd-b9fe-c51c14816310",
   "metadata": {},
   "source": [
    "# Building your first Agent step-by-step with h2oGPTe & LLM Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7c8ffa-badd-424b-9760-6e18f1628751",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Description: A technical walkthrough of how to build Custom Agents with step by step with Enterprise h2oGPTe and LLM Chains\n",
    "\n",
    "**In this blog, you will learn about LLM Chains, tools or functions calling, agents and how to leverage and customise h2oGPTe to create your first agent, all while benefiting from privately hosted LLMs, and secured data that stays with you(+)**. \n",
    "\n",
    "(+) as customer of enterprise h2oGPTe.\n",
    "\n",
    "### Pre-requisites:\n",
    "- Access & register https://h2ogpte.genai.h2o.ai/ \n",
    "- Python coding knowledge, virtual env installation\n",
    "- Python > 3.6 installed, ability to install libraries through pip:\n",
    "    - pip install h2ogpte==1.5.20\n",
    "    - pip install langchain\n",
    "    - pip install langchain_core\n",
    "- Installed Jupyter Notebook :\n",
    "    - https://docs.jupyter.org/en/latest/install/notebook-classic.html-\n",
    "    - https://jupyterlab.readthedocs.io/en/stable/getting_started/installation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee928dc-641d-43b2-9274-853944f4ebd6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## *The basics: what are LLMs chains, tools and Agents?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f3227b-cc38-426a-a63f-b2b113d13b42",
   "metadata": {
    "tags": []
   },
   "source": [
    "### LLM Chaining\n",
    "\n",
    "LLM chaining or Large Language Model Chaining is the process of integrating one or multiple large language models - such as the ones hosted on h2oGPTe - with other applications, tools, and services. These chains or pipelines allow a language model to leverage the strengths of other tools and services, and to overcome its own limitations, in order to generate the most effective output possible.\n",
    "\n",
    "A chain is like a pipeline that processes an input by using a specific combination of components: tools, LLMs, services, parsers. It can be thought of \n",
    "as a sequence of processing 'steps' that performs a certain set of operations on an input and returns the result:\n",
    "For example, an LLM might be chained with an API endpoint in order to fetch real-time information on stock market or the news (... or the score of a predictive model).\n",
    "\n",
    "The goal is to create a more powerful and versatile AI system (such as personal assistants that can provide accurate and useful outputs to a various range of inputs. \n",
    "\n",
    "### LLM Chaining frameworks\n",
    "To date, there are several Open Source LLM Chaining frameworks : ***AutoGen, Langchain, LLamaIndex, Haystack***. Today, I will be using **Langchain** as it may be (arguably) the most flexible, versatile, and intuitive open source framework to use. \n",
    "\n",
    "### Tools\n",
    "\n",
    "Tools or functions (or functions calling) are specialized functionalities that can accomplish a specific task given a set of inputs and are interfaces that an agent, chain, or LLM can use to interact with.\n",
    "\n",
    "### Agents\n",
    "\n",
    "Agents are systems that use LLMs as reasoning engines to decide which actions to take, tools to use and the inputs to pass them. After executing an action (like using a tool and collecting the result from the tool), the results can be fed back into the LLM to assess if further actions are required or if the process can be concluded.\n",
    "\n",
    "**In summary**:\n",
    "\n",
    "- Chains are sequences of processing steps for prompts. They are used within agents to define how the agent processes information.\n",
    "- Tools are specialized functionalities that are used by agents or within chains for specific tasks.\n",
    "- Agents are like characters with specific capabilities which use chains and tools to perform their functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825238e4-ca34-439d-a579-eef158916780",
   "metadata": {},
   "source": [
    "## *what about Enterprise h2oGPTe?*\n",
    "\n",
    "Enterprise h2oGPTe is a customisable search assistant that helps you find answers to questions about your documents, websites, or workplace content through its user interface or its python API. It is automatically able to contextualize its response with your own data (whether these are text, images, or audio) using various RAG (Retrieval-Augmented Generation) approaches or answer simply a general question. \n",
    "\n",
    "\n",
    "### Why creating your own agent with h2oGPTe?\n",
    "\n",
    "Enterprise h2oGPTe will soon be releasing a version of its software integrating with its own agents and more functionnalities. Creating your own agent is fun way to learn how it works, when they are useful and how they may be useful your own use case or applications! In addition, it helps you to get started with h2oGPTe API using its public version (and it is free!).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355e2d94-7c95-4f88-8ff9-18c39e912633",
   "metadata": {},
   "source": [
    "## Now... Let's get started and put the theory into practise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5e3003-96d8-4289-9012-6117591924be",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Connect to H2o h2oGPTe platform: Visit our [public h2oGPTe](https://h2oGPTe.genai.h2o.ai/) and follow along !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d050651-eb59-47d8-9d2e-7f0b42ea8390",
   "metadata": {
    "tags": []
   },
   "source": [
    "You can get started by creating [your own API key](https://h2ogpte.genai.h2o.ai/settings) using the public version of Enterprise h2oGPTe (not for production systems).\n",
    "You can also check out some of the cool applications created using h2oGPTe in the background: https://genai.h2o.ai/appstore  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef81bc17-0962-44ae-974e-f5660f38a170",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from h2ogpte import H2OGPTE\n",
    "\n",
    "H2OGPTE_KEY =\"< your api key> \"\n",
    "\n",
    "h2ogpte_client = H2OGPTE(\n",
    "    address='https://h2ogpte.genai.h2o.ai/',\n",
    "    api_key=H2OGPTE_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de7db7c-237e-4209-98e7-2a9a80a6f779",
   "metadata": {},
   "source": [
    "## 1 - Integrate h2oGPTe tools and functionnalities and LangChain framework\n",
    "\n",
    "To combine h2oGPTe tools and functionnalities and LangChain framework, we will first start by wrapping our h2oGPTe client as a [LangChain Chat Model](https://python.langchain.com/v0.2/docs/concepts/#chat-models) that uses chat messages as inputs and returns chat messages as outputs (that are not just plain text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bd9ebc1-7399-4f9d-bd77-663310d6a6a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Any, AsyncIterator, Dict, Iterator, List, Optional\n",
    "import json\n",
    "\n",
    "from langchain_core.callbacks import CallbackManagerForLLMRun\n",
    "\n",
    "from langchain_core.messages import AIMessageChunk, BaseMessage, HumanMessage, AIMessage, SystemMessage,FunctionMessage\n",
    "from langchain_core.outputs import ChatResult, ChatGeneration\n",
    "from langchain.schema.runnable import RunnableMap\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "class h2ogpteChatModel(BaseChatModel):\n",
    "    \"\"\"h2oGPTeChatModel Class based on LangChain BaseChatModel Class.\n",
    "    Example:\n",
    "        .. code-block:: python\n",
    "\n",
    "            h2ogpte = h2ogpteChatModel(h2ogpte_client = h2ogpte_client, model_name = \"h2oai/h2o-danube3-4b-chat\")\n",
    "            result = model.invoke([HumanMessage(content=\"hello\")])\n",
    "            result = model.batch([[HumanMessage(content=\"hello\")],\n",
    "                                 [HumanMessage(content=\"world\")]])\n",
    "    \"\"\"\n",
    "    \n",
    "    h2ogpte_client: H2OGPTE\n",
    "    model_name: Optional[str]\n",
    "    collection_id: Optional[str]\n",
    "    kwargs: Optional[dict]\n",
    "\n",
    "    def _generate(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> ChatResult:\n",
    "        \"\"\"Pass on Prompt input to h2oGPTe via its client, create a chat session and generate outputs given this input\n",
    "\n",
    "        Args:\n",
    "            messages: the prompt composed of input messages\n",
    "            stop: a list of strings on which the model should stop generating.\n",
    "                  If generation stops due to a stop token, the stop token itself\n",
    "                  SHOULD BE INCLUDED as part of the output. This is not enforced\n",
    "                  across models right now, but it's a good practice to follow since\n",
    "                  it makes it much easier to parse the output of the model\n",
    "                  downstream and understand why generation stopped.\n",
    "            run_manager: A run manager with callbacks for the LLM.\n",
    "        \"\"\"\n",
    "        \n",
    "        prompt = ChatPromptTemplate(messages=messages)\n",
    "        messages_as_string = prompt.format()\n",
    "        if not self.model_name:\n",
    "            self.model_name = self.h2ogpte_client.get_llms()[0]['display_name']\n",
    "            \n",
    "        if not self.collection_id:\n",
    "            self.collection_id = None\n",
    "        \n",
    "        chat_session_id = h2ogpte_client.create_chat_session(self.collection_id)\n",
    "        with h2ogpte_client.connect(chat_session_id) as session:\n",
    "            response = session.query(message = messages_as_string,\n",
    "                                         llm = self.model_name, \n",
    "                                         **kwargs).content\n",
    "                \n",
    "            responseAIMessage = AIMessage(\n",
    "                                    content=response,\n",
    "                                    additional_kwargs={},\n",
    "                                    response_metadata={},\n",
    "                                )\n",
    "\n",
    "        return ChatResult(generations=[ChatGeneration(message=responseAIMessage)])\n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Get the type of language model used by this chat model.\n",
    "        Used to uniquely identify the type of the model. Used for logging.\"\"\"\n",
    "        return self.model_name\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"Return a dictionary of identifying parameters.\n",
    "        Represent model parameterization for tracing purposes.\n",
    "        \"\"\"\n",
    "        return [model for model in self.h2ogpte_client.get_llms() if model[\"display_name\"] == self.model_name][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff30e33-3935-44a0-9bc9-086ae21d8a77",
   "metadata": {},
   "source": [
    "h2ogpteChatModel takes as input an instance of h2oGPTe client, an optionally the llm *model_name* the user would like to pick from h2ogpte as well as the *collection_ID* or string ID of the collection of documents to chat with using Retrieval Augmented Generation (RAG). In parallel, Enterprise h2oGPTe has a highly customizable prompting to talk to LLM, document & collection of documents, summarise and extract information and is...  LLM agnostic - you can choose the model you need for your use case, including your own fine tuned model! \n",
    "\n",
    "The *_generate* method is used to generate a chat result from a prompt: we create a chat session with h2ogpte, pass the user prompt (provide additional optional arguments) and we return a ChatResult of the response generated from h2ogpte chat session as AIMessage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59d0884-d88c-407e-b4c8-0369a97a5a80",
   "metadata": {},
   "source": [
    "One of the model we can use is our own H2o series of small language models (H2O-Danube3-4B & H2O-Danube3-500M), fined-tuned for conversation using H2O LLM Studio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "575bfcaf-2246-433e-8785-025259d665e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'h2oai/h2o-danube3-4b-chat'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = None\n",
    "\n",
    "if \"h2oai/h2o-danube3-4b-chat\" in [llm['display_name'] for llm in h2ogpte_client.get_llms()]:\n",
    "    model_name = 'h2oai/h2o-danube3-4b-chat'\n",
    "model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb6d42a-ea5c-459b-bb99-59d7b504a614",
   "metadata": {},
   "source": [
    "<!-- ## TODO WORD ON DANUBE -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56e85eb9-a0a3-4314-854d-92b9f01b7266",
   "metadata": {},
   "outputs": [],
   "source": [
    "h2ogpteChat = h2ogpteChatModel(h2ogpte_client = h2ogpte_client, model_name = model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4dec21-14b4-4c0a-ab37-027762d240a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "In addition to the model, we can include parameters of the RAG, and the LLM arguments such as temperature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ffe2df3-0113-434b-a815-28f075758063",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello! I'm here to help you with any questions or information you need. How can I assist you today?\", id='run-5c745867-13bd-4904-a6da-cd51bc7f766c-0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h2ogpteChat.invoke(\"whatsup\", llm_args = {\"temperature\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "867f8350-a5fc-492e-a7e9-2bb5ba985099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Interesting question! As an AI, I don't have feelings, but I'm here to help answer your questions and provide information in ways that I'm programmed to do.\", id='run-47cd58fa-a73d-4186-ab87-357b04fff30c-0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h2ogpteChat.invoke(\"whatsup\", llm_args = {\"temperature\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f596a1-7633-4131-a503-5777c319eaf7",
   "metadata": {
    "tags": []
   },
   "source": [
    "##\n",
    "#### Now, we have created our first component of an LLM Chain (h2ogpteChatModel), let's create our first chain with \"Langchain Expression language\" or LCEL.\n",
    "\n",
    "To do so, we will start by adding to our chat Model one of the simplest component of a chain: Prompt template. it will help to format the user input and provides a consistent and standardized way to present the prompt to the LLM.\n",
    "\n",
    "Please note, h2oGPTe has its own prompt template catalog, that you can augment with your own defined template for your use case. Find out more [here](https://docs.h2o.ai/enterprise-h2ogpte/blog/tags/v-1-4-13#introducing-the-prompt-catalog).\n",
    "\n",
    "Next, the prompt is then passed to the LLM component of the chain: h2oGPTe Chat which will processe the input prompt and generates a response accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbb6631f-0cff-4220-848d-dfb191014ded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Audrey! I'm an AI assistant designed to help answer your questions and provide information. How can I assist you today?\", id='run-0e6f9eea-b6c4-45ce-ad9a-22ce9df864b1-0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"My name is Audrey, {input}\"\n",
    ")\n",
    "chain = prompt | h2ogpteChat\n",
    "\n",
    "chain.invoke({\"input\":\"hello, how are you?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b2ed25-df6f-4404-8b91-8a823d8f471a",
   "metadata": {},
   "source": [
    "\n",
    "That's it ! very easy, we have created our first chain!\n",
    "\n",
    "##\n",
    "Now, depending on the requirements and objectives of the downstream application this chain is used for, the response from h2ogpteChat - the last step on our current chain - can be displayed to the user, further processed, or fed into the next component in the chain. \n",
    "\n",
    "#### Let's add a simple String Output parser to our chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51740a88-9d66-428a-a15d-a2227932e3dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Audrey! I'm an AI assistant designed to help answer your questions and provide information. How can I assist you today?\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "chain = prompt | h2ogpteChat | StrOutputParser()\n",
    "\n",
    "chain.invoke({\"input\":\"hello, how are you?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f475a957-ea67-4d72-8cd4-8804c81d3097",
   "metadata": {},
   "source": [
    "\n",
    "#### Now, would it not be great if our h2ogpteChat model could interact with user defined tools, APIs to fetch some data, perform some actions whenever the user or an application is requesting it?\n",
    "\n",
    "This is possible by binding functions or tools to our h2ogpteChat model to choose from. \n",
    "\n",
    "First, we need to declare our functions definition. Similarly to what can be found in function dosctrings, we need to describe our function in a json schema: description, parameters that are required to call the function along with their types and description. \n",
    "\n",
    "\n",
    "Let's say we have two functions called *get_country_information* and *get_current_weather* defined as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "549aaf3b-7757-4859-a774-d0b75078e118",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'get_country_information',\n",
       "  'description': 'the function get_country_information can be used to Get information about a country such as: its capital, currency, population or maps',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'country': {'type': 'string',\n",
       "     'description': 'The country of interest, for example Italy'},\n",
       "    'field_to_extract': {'type': 'string',\n",
       "     'enum': ['capital', 'currency', 'population', 'maps']}},\n",
       "   'required': ['country', 'field_to_extract']}},\n",
       " {'name': 'get_current_weather',\n",
       "  'description': 'the function get_current_weather can be used to get the current temperature in Celsius or Fahrenheit in a given location (City, State)',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'location': {'type': 'string',\n",
       "     'description': 'The city and state, e.g. San Francisco, CA'},\n",
       "    'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}},\n",
       "   'required': ['location']}}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "functions = [\n",
    "        {\n",
    "            \"name\": \"get_country_information\",\n",
    "            \"description\": \"the function get_country_information can be used to Get information about a country such as: its capital, currency, population or maps\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"country\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The country of interest, for example Italy\",\n",
    "                    },\n",
    "                    \"field_to_extract\": {\"type\": \"string\", \"enum\": [\"capital\", \"currency\", \"population\", \"maps\"]},\n",
    "                },\n",
    "                \"required\": [\"country\", \"field_to_extract\"],\n",
    "            },\n",
    "        },\n",
    "    \n",
    "    {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"the function get_current_weather can be used to get the current temperature in Celsius or Fahrenheit in a given location (City, State)\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                    },\n",
    "                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            },\n",
    "        },\n",
    "\n",
    "  ]\n",
    "functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b232bd-bff8-455a-a3ab-f460d55869e3",
   "metadata": {},
   "source": [
    "##\n",
    "\n",
    "By binding the functions defined above to h2ogpteChat model class, we are letting it know what functions we have access to each time the model is invoked, what they are for and how to appropriately invoke them using this json schema. This way, the LLM can determine based on the user query to invoke those tools if necessary.\n",
    "\n",
    "Let's see now on a working example how we achieve that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02fae066-378f-4446-b09f-0f654abd0889",
   "metadata": {},
   "outputs": [],
   "source": [
    "h2ogpteChat_bind = h2ogpteChat.bind(functions=functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81b4b683-9432-468f-974f-9c7b150cae31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "h2ogpteChatModel(h2ogpte_client=<h2ogpte.h2ogpte.H2OGPTE object at 0x7fe200704790>, model_name='h2oai/h2o-danube3-4b-chat')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h2ogpteChat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "040e122e-ad06-487d-bacd-bd3c99089770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=h2ogpteChatModel(h2ogpte_client=<h2ogpte.h2ogpte.H2OGPTE object at 0x7fe200704790>, model_name='h2oai/h2o-danube3-4b-chat'), kwargs={'functions': [{'name': 'get_country_information', 'description': 'the function get_country_information can be used to Get information about a country such as: its capital, currency, population or maps', 'parameters': {'type': 'object', 'properties': {'country': {'type': 'string', 'description': 'The country of interest, for example Italy'}, 'field_to_extract': {'type': 'string', 'enum': ['capital', 'currency', 'population', 'maps']}}, 'required': ['country', 'field_to_extract']}}, {'name': 'get_current_weather', 'description': 'the function get_current_weather can be used to get the current temperature in Celsius or Fahrenheit in a given location (City, State)', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}]})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h2ogpteChat_bind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8940f8-fdd5-403c-85ba-c60b86310c30",
   "metadata": {},
   "source": [
    "### Modifying the h2ogpteChatModel class to add h2oGPTe guided generation for Function calling\n",
    "Now, we just need to give some instructions to our h2ogpteChat model to:\n",
    "\n",
    "- make it aware of the functions available\n",
    "- assess when and if these functions are necessary given an input prompt\n",
    "- respond in a consistent manner that will allow downstream to call the function appropriately (in our case convert natural language into valid API calls)\n",
    "\n",
    "For this, we are adding the *_function_calling_prompt* property:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3ba86a-d135-4434-84b0-4266610834a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "    @property\n",
    "    def _function_calling_prompt(self) -> str:\n",
    "        \"\"\"Get the function calling prompt for the LLM\"\"\"\n",
    "        return  \"\"\" // namespace functions\n",
    "        You serve as a wrapper for utilizing multiple tools. Each tool that can be used MUST be specified in the list of the namespace functions section above ONLY. Ensure that the parameters provided to each tool are valid according to that tool's specification. Only functions in the functions namespace are permitted, \n",
    "        DO NOT GUESS or MAKE UP a function if it is not available in the list of namespace functions section above.\n",
    "        Their description, properties are described. Given the query given below, respond with the function names to use, list of arguments and their value in a valid json. \n",
    "        For example, if the instruction are matching one or more of the description of the function above:\n",
    "\n",
    "        {'content': '', 'additional_kwargs': {'function_call': {'name': '<name of relevant listed function_name>', 'arguments': '{\"<name of argument_1 of listed function>\": \"<argument 1 value>\", \"<name of argument_2 of listed function>\": \"<argument 2 value>\"}' }}}\n",
    "\n",
    "        if none of the functions help with the query, are matching or relevant to the query/instructions below, DO NOT include the functions and simply respond instead in the below format to the best of your knowledge of the question in a valid json:\n",
    "\n",
    "        {'content': '<response>', 'additional_kwargs': {}}\n",
    "        \n",
    "        here is the query, please respond:\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0548039c-1d81-49ae-ab79-6b784ef82392",
   "metadata": {},
   "source": [
    "We augment the *_generate* method to use *_function_calling_prompt*, check if a function call is relevant and leverage h2ogpte [guided generation](https://docs.h2o.ai/enterprise-h2ogpte/blog/v1.5#guided-generation) by providing the json schema we require the h2ogpteChat model to return IF a function call is deemed necessary given a user input:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219d1054-f91d-4feb-ab55-8b78dec0606f",
   "metadata": {},
   "source": [
    "<!-- ## TODO WORD ON GUIDED GENERATION -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b249393d-6a67-450d-a56d-a113a4e2b835",
   "metadata": {},
   "outputs": [],
   "source": [
    "    if 'functions' in kwargs:\n",
    "        requires_function = session.query(message = messages_as_string,\n",
    "                                                     system_prompt = f\"If Only the following functions are available: {str(kwargs['functions'])}, do any of the listed functions are relevant in answering entirely the following user query? Respond ONLY with True or False. user query:\",\n",
    "                                 llm = self.model_name).content\n",
    "        if \"true\" in str.lower(requires_function):\n",
    "            function_calling_prompt = f\"\"\"## functions: \\n namespace functions \\\\ \\n {str(kwargs[\"functions\"])} \"\"\" + self._function_calling_prompt\n",
    "            response = session.query(message = messages_as_string,\n",
    "                                     system_prompt = function_calling_prompt,\n",
    "                                     llm = self.model_name,\n",
    "                                    ).content\n",
    "            response = session.query(message = f'''\n",
    "                                    Return a valid JSON\n",
    "                                    Escape any backslasheswith a backslash.\n",
    "                                    Escape any double quotes with \\\\.\n",
    "                                    Escape any newline characters so that they become \\\\n.\n",
    "                                    The response must be a valid JSON. \n",
    "\n",
    "                                    Please review : {response}\n",
    "                                     ''',\n",
    "                                     system_prompt = function_calling_prompt,\n",
    "                                     llm = self.model_name,\n",
    "                                     llm_args=dict(\n",
    "                                        response_format='json_object',\n",
    "                                        guided_json=self.gjson\n",
    "                                 )).content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7804dcc-9480-470e-945f-f12db1c24453",
   "metadata": {},
   "source": [
    "Here is the final Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9822de7b-026b-4103-9701-0084db3c4c4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class h2ogpteChatModel(BaseChatModel):\n",
    "    \"\"\"A custom chat model that augment LangChain BaseChatModel Class.\n",
    "    Example:\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            h2ogpte = h2ogpteChatModel(h2ogpte_client = h2ogpte_client, model_name = \"h2oai/h2o-danube3-4b-chat\")\n",
    "            result = model.invoke([HumanMessage(content=\"hello\")])\n",
    "            result = model.batch([[HumanMessage(content=\"hello\")],\n",
    "                                 [HumanMessage(content=\"world\")]])\n",
    "    \"\"\"\n",
    "    \n",
    "    h2ogpte_client: H2OGPTE\n",
    "    \"h2ogpte client to connect to Enterprise h2oGPTe and tools \"\n",
    "    model_name: Optional[str]\n",
    "    collection_id: Optional[str]\n",
    "    gjson: Optional[dict] = {\n",
    "                             \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n",
    "                             \"type\": \"object\",\n",
    "                              \"properties\": {\n",
    "                                \"content\": {\n",
    "                                  \"type\": \"string\"\n",
    "                                },\n",
    "                                \"additional_kwargs\": {\n",
    "                                  \"type\": \"object\",\n",
    "                                  \"default\": {},\n",
    "                                  \"properties\": {\n",
    "                                    \"function_call\": {\n",
    "                                     \"type\": \"object\",\n",
    "                                     \"default\": {},\n",
    "                                    \"properties\": {\n",
    "                                           \"name\": {\n",
    "                                             \"type\": \"string\"\n",
    "                                            },\n",
    "                                            \"arguments\": {\n",
    "                                                \"type\": \"string\",\n",
    "                                            }\n",
    "                                        }\n",
    "                                    }\n",
    "                                },\n",
    "                                \"required\": [] \n",
    "                                }\n",
    "                              },\n",
    "                              \"required\": [ \"content\", \"additional_kwargs\"]\n",
    "                            }\n",
    "    kwargs: Optional[dict]\n",
    "\n",
    "    def _generate(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> ChatResult:\n",
    "        \"\"\"Pass on Prompt input to h2oGPTe client to creat a chat session and generate outputs given an input.\n",
    "\n",
    "        Args:\n",
    "            messages: the prompt composed of input messages\n",
    "            stop: a list of strings on which the model should stop generating.\n",
    "                  If generation stops due to a stop token, the stop token itself\n",
    "                  SHOULD BE INCLUDED as part of the output. This is not enforced\n",
    "                  across models right now, but it's a good practice to follow since\n",
    "                  it makes it much easier to parse the output of the model\n",
    "                  downstream and understand why generation stopped.\n",
    "            run_manager: A run manager with callbacks for the LLM.\n",
    "        \"\"\"\n",
    "\n",
    "        prompt = ChatPromptTemplate(messages=messages)\n",
    "        messages_as_string = prompt.format()\n",
    "\n",
    "        if not self.model_name:\n",
    "            self.model_name = self.h2ogpte_client.get_llms()[0]['display_name']\n",
    "        if not self.collection_id:\n",
    "            self.collection_id = None\n",
    "        \n",
    "        chat_session_id = h2ogpte_client.create_chat_session(self.collection_id)\n",
    "        with h2ogpte_client.connect(chat_session_id) as session:\n",
    "            if 'functions' in kwargs:\n",
    "                \n",
    "                is_answered=\"False\"\n",
    "                if len(messages)>1:\n",
    "                    is_answered = session.query(message = prompt[1:].format(),\n",
    "                                                system_prompt = f\"Is the response to << {messages[0].content} >> been answered by the below messages only, answer with True or False:\",\n",
    "                                                llm = self.model_name).content\n",
    "                    \n",
    "                requires_function = session.query(message = messages_as_string,\n",
    "                                                            system_prompt = f\"Only the following functions are available: {str(kwargs['functions'])}, are any of the listed functions relevant in answering entirely the following user query? Respond ONLY with True if relevants, or False if not. user query:\",\n",
    "                                                            llm = self.model_name).content\n",
    "                \n",
    "                if \"true\" in str.lower(requires_function) and \"false\" in str.lower(is_answered):\n",
    "                    function_calling_prompt = f\"\"\"## functions: \\n namespace functions // \\n {str(kwargs[\"functions\"])} \"\"\" + self._function_calling_prompt\n",
    "                    response = session.query(message = messages_as_string,\n",
    "                                             system_prompt = function_calling_prompt,\n",
    "                                             llm = self.model_name,\n",
    "                                            ).content\n",
    "                    \n",
    "                    #  guided json/generation\n",
    "                    response = session.query(message = f'''\n",
    "                                            Return a valid JSON\n",
    "                                            Escape any backslashes with a backslash.\n",
    "                                            The response must be a valid JSON. \n",
    "                                            \n",
    "                                            Please review : {response}\n",
    "                                             ''',\n",
    "                                             system_prompt = function_calling_prompt,\n",
    "                                             llm = self.model_name,\n",
    "                                             llm_args=dict(\n",
    "                                                response_format='json_object',\n",
    "                                                guided_json=self.gjson\n",
    "                                             )).content\n",
    "                    try:\n",
    "                        json_response = json.loads(response)\n",
    "                        if json_response['additional_kwargs']['function_call']['name'] not in [fct['name'] for fct in functions]:\n",
    "                            json_response['additional_kwargs'] = {}\n",
    "\n",
    "                        responseAIMessage = AIMessage(\n",
    "                            content=json_response['content'],\n",
    "                            additional_kwargs=json_response['additional_kwargs'],\n",
    "                            response_metadata={},\n",
    "                        )\n",
    "                    except:\n",
    "                        print(\"failed to extract response['content']\")\n",
    "                        responseAIMessage = AIMessage(\n",
    "                            content=response,\n",
    "                            additional_kwargs={},\n",
    "                            response_metadata={},\n",
    "                        )\n",
    "                elif \"true\" in str.lower(is_answered):\n",
    "                    responseAIMessage = AIMessage(content=[message.content for message in messages if message.type==\"function\"][-1],)\n",
    "                else:\n",
    "                    kwargs.pop(\"functions\", None)\n",
    "                    response = session.query(messages_as_string,\n",
    "                                             llm = self.model_name, \n",
    "                                             **kwargs\n",
    "                                            ).content\n",
    "                    responseAIMessage = AIMessage(\n",
    "                                        content=response,\n",
    "                                        additional_kwargs={},\n",
    "                                        response_metadata={},\n",
    "                                    )\n",
    "            else:\n",
    "                kwargs.pop(\"functions\", None)\n",
    "                response = session.query(messages_as_string,\n",
    "                                         llm = self.model_name, **kwargs\n",
    "                                        ).content\n",
    "                responseAIMessage = AIMessage(\n",
    "                                    content=response,\n",
    "                                    additional_kwargs={},\n",
    "                                    response_metadata={},\n",
    "                                )\n",
    "        generation = ChatGeneration(message=responseAIMessage)\n",
    "        return ChatResult(generations=[generation])\n",
    "\n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Get the type of language model used by this chat model.\n",
    "        Used to uniquely identify the type of the model. Used for logging.\"\"\"\n",
    "        return self.model_name\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"Return a dictionary of identifying parameters.\n",
    "        Represent model parameterization for tracing purposes.\n",
    "        \"\"\"\n",
    "        return [model for model in self.h2ogpte_client.get_llms() if model[\"display_name\"] == self.model_name][0]\n",
    "    @property\n",
    "    def _function_calling_prompt(self) -> str:\n",
    "        \"\"\"Get the function calling prompt for the LLM\"\"\"\n",
    "        return  \"\"\" // namespace functions\n",
    "        You serve as a wrapper for utilizing multiple tools. Each tool that can be used MUST be specified in the list of the namespace functions section above ONLY. Ensure that the parameters provided to each tool are valid according to that tool's specification. Only functions in the functions namespace are permitted, \n",
    "        DO NOT GUESS or MAKE UP a function if it is not available in the list of namespace functions section above.\n",
    "        Their description, properties are described. Given the query given below, respond with the function names to use, list of arguments and their value in a valid json. \n",
    "        For example, if the instruction are matching one or more of the description of the function above:\n",
    "\n",
    "        {'content': '', 'additional_kwargs': {'function_call': {'name': '<name of relevant listed function_name>', 'arguments': '{\"<name of argument_1 of listed function>\": \"<argument 1 value>\", \"<name of argument_2 of listed function>\": \"<argument 2 value>\"}' }}}\n",
    "\n",
    "        if none of the functions help with the query, are matching or relevant to the query/instructions below, DO NOT include the functions and simply respond instead in the below format to the best of your knowledge of the question in a valid json:\n",
    "\n",
    "        {'content': '<response>', 'additional_kwargs': {}}\n",
    "        \n",
    "        here is the query, please respond:\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00b26ddf-e6be-41b6-ad25-5627eeb34923",
   "metadata": {},
   "outputs": [],
   "source": [
    "h2ogpteChat = h2ogpteChatModel(h2ogpte_client = h2ogpte_client, model_name = 'mistralai/Mixtral-8x7B-Instruct-v0.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6025b3cd-ddcd-45ed-9288-7eb42d735307",
   "metadata": {},
   "outputs": [],
   "source": [
    "h2ogpteChat_bind = h2ogpteChat.bind(functions=functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13301505-2019-4e01-9f87-16b5cab4725b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm an AI model, so I don't have feelings, but I'm here and ready to help you with any questions or problems you have to the best of my ability! How can I assist you today?\", id='run-716e957d-8de6-4f9b-80ed-28a41c67c43d-0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h2ogpteChat.invoke(\"how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78dcd460-6f57-464b-96b6-6c5a05176c4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm an AI model, so I don't have feelings, but I'm here and ready to help you with any questions or problems you have to the best of my ability! How can I assist you today?\", id='run-364f791f-3499-4fee-9de0-024c22141164-0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h2ogpteChat_bind.invoke(\"how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5de570ba-8e1a-44be-8cb3-330b596fd011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='My name is Audrey, {input}'))])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1173782-c23c-47e7-800d-9b7ac0bb67c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | h2ogpteChat_bind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ded2df9-d61e-4f92-a410-016599c14464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Audrey, I'm h2oGPTe. The cost of living can vary greatly depending on individual lifestyle, but on average, to live comfortably in San Francisco, it's often suggested that you might need a salary of around $120,000 to $130,000 per year. This takes into account the high cost of housing, healthcare, transportation, and other expenses. However, please note that this is a general estimate and individual needs can vary.\", id='run-500eb15a-fc13-41ca-862f-1068c4542392-0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"how much money do you need to earn to live comfortably in San Francisco?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742432a4-1463-4ae4-a0d9-fdc63fcb7536",
   "metadata": {},
   "source": [
    "Here the h2ogpteChatModel deemed based on my instrutions that a function call was not necessary and used its own internal knowledge to generate a general answer to my question. \n",
    "\n",
    "*What if I ask questions relevant to the functions I declare and bind to my h2ogpteChatModel?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e31170b2-87d1-4e3a-a2a6-d7eff06e74a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_current_weather', 'arguments': '{\"location\": \"San Francisco, CA\", \"unit\": \"fahrenheit\"}'}}, id='run-91276a03-63ad-4bae-b928-68d8b08730c1-0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"hello, what is the temperature in San Francisco?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df6d949b-19bc-48c6-9e7a-3242861273e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_country_information', 'arguments': '{\"country\": \"Albania\", \"field_to_extract\": \"population\"}'}}, id='run-0d1b32c4-02b3-47b5-a511-e16fe0a3a3f9-0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"what is the number of inhabitants in Albania?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76a589a3-c2a5-4cb7-88b5-3d9bfff889ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_country_information', 'arguments': '{\"country\": \"Turkey\", \"field_to_extract\": \"currency\"}'}}, id='run-b84a82fc-add8-4bfc-b1eb-4f3ca2680f83-0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"what is the currency in Turkey?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6931fbfb-7f8a-4bda-9550-5879b6df5933",
   "metadata": {},
   "source": [
    "Now the outputs are consisting of the argument *additional_kwargs* which contains the relevant function name, arguments and associated value, that is, all the informations necessary to call the function required to obtain the answer to the user query!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec9541d-7362-433a-8f54-c40fea2bc4ad",
   "metadata": {},
   "source": [
    "### Great, now let's go a step further so we can execute the underlying relevant function and return the output to the user when relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d4184aa0-d11e-43d6-acf5-913b7f13b6b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from typing import Literal\n",
    "from pydantic.v1 import BaseModel, Field\n",
    "import datetime\n",
    "from langchain.agents import tool\n",
    "\n",
    "# Define the input schema\n",
    "class OpenCountryInfo(BaseModel):\n",
    "    country: str = Field(..., description=\"Name of the country to fetch data for. For example 'Italy'\")\n",
    "    field_to_extract: Literal['capital', 'currency', 'population', 'maps'] = Field(..., description=\"Name of the field or information to extract data about\")\n",
    "    \n",
    "@tool(args_schema=OpenCountryInfo)\n",
    "def get_country_information(country:str, field_to_extract:str) -> dict:\n",
    "    \"\"\"this function is used to extract information about a country such as: capital, currency, population OR maps\"\"\"\n",
    "    \n",
    "    BASE_URL = f\"https://restcountries.com/v3.1/name/{country}/?fields={field_to_extract}\"\n",
    "    response = requests.get(BASE_URL)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        results = response.json()\n",
    "    else:\n",
    "        raise Exception(f\"API Request failed with status code: {response.status_code}\")\n",
    "    return f\"{country}'s {field_to_extract} is {list(results[0].values())[0]}\"\n",
    "\n",
    "# Define the input schema\n",
    "class OpenMeteoInput(BaseModel):\n",
    "    latitude: float = Field(..., description=\"Latitude of the location to fetch weather data for\")\n",
    "    longitude: float = Field(..., description=\"Longitude of the location to fetch weather data for\")\n",
    "\n",
    "@tool(args_schema=OpenMeteoInput)\n",
    "def get_current_temperature(latitude: float, longitude: float) -> dict:\n",
    "    \"\"\"Fetch current temperature for given coordinates.\"\"\"\n",
    "    \n",
    "    BASE_URL = \"https://api.open-meteo.com/v1/forecast\"\n",
    "    \n",
    "    # Parameters for the request\n",
    "    params = {\n",
    "        'latitude': latitude,\n",
    "        'longitude': longitude,\n",
    "        'hourly': 'temperature_2m',\n",
    "        'forecast_days': 1,\n",
    "    }\n",
    "\n",
    "    # Make the request\n",
    "    response = requests.get(BASE_URL, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        results = response.json()\n",
    "    else:\n",
    "        raise Exception(f\"API Request failed with status code: {response.status_code}\")\n",
    "\n",
    "    current_utc_time = datetime.datetime.utcnow()\n",
    "    time_list = [datetime.datetime.fromisoformat(time_str.replace('Z', '+00:00')) for time_str in results['hourly']['time']]\n",
    "    temperature_list = results['hourly']['temperature_2m']\n",
    "    \n",
    "    closest_time_index = min(range(len(time_list)), key=lambda i: abs(time_list[i] - current_utc_time))\n",
    "    current_temperature = temperature_list[closest_time_index]\n",
    "    \n",
    "    return f'The current temperature is {current_temperature}°C'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c1d3941a-1d6d-4991-b76a-267e9411c70b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "\n",
    "functions = [\n",
    "    convert_to_openai_function(f) for f in [\n",
    "        get_current_temperature,\n",
    "        get_country_information,\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "baf1f062-e2ea-4d1e-af47-17507c01b343",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'get_current_temperature',\n",
       "  'description': 'Fetch current temperature for given coordinates.',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'latitude': {'description': 'Latitude of the location to fetch weather data for',\n",
       "     'type': 'number'},\n",
       "    'longitude': {'description': 'Longitude of the location to fetch weather data for',\n",
       "     'type': 'number'}},\n",
       "   'required': ['latitude', 'longitude']}},\n",
       " {'name': 'get_country_information',\n",
       "  'description': 'this function is used to extract information about a country such as: capital, currency, population OR maps',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'country': {'description': \"Name of the country to fetch data for. For example 'Italy'\",\n",
       "     'type': 'string'},\n",
       "    'field_to_extract': {'description': 'Name of the field or information to extract data about',\n",
       "     'enum': ['capital', 'currency', 'population', 'maps'],\n",
       "     'type': 'string'}},\n",
       "   'required': ['country', 'field_to_extract']}}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f7a5b8-19c1-46ef-b3f3-d59c95bcde51",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Great, now let's go a step further so we can execute the underlying relevant function and return the output to the user when relevant.\n",
    "\n",
    "First, we will be borrowing the [OpenAIFunctionsAgentOutputParser](https://api.python.langchain.com/en/latest/_modules/langchain/agents/output_parsers/openai_functions.html#OpenAIFunctionsAgentOutputParser), which simply check if the output from our h2ogpteChat Model contains a *function_call* argument or not and define wether an action such as executing a function call is required from the agent (***AgentAction***) or the agent job is done here (***AgentFinish***) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "421e5957-9588-41ca-8cc4-a71975b5d48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "from langchain_core.agents import AgentAction, AgentActionMessageLog, AgentFinish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4760ecd3-5d9d-4356-9d68-1f442fa35bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "h2ogpteChat = h2ogpteChatModel(h2ogpte_client = h2ogpte_client, model_name = 'mistralai/Mixtral-8x7B-Instruct-v0.1')\n",
    "h2ogpteChat_bind = h2ogpteChat.bind(functions=functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88721d2-746e-4712-9929-0633f91299a7",
   "metadata": {},
   "source": [
    "now we can define a route function, which will check if the agent has reached a final (*AgentFinish*) status and return the final answer or will route the query towards the appropriate tools (*result.tool*) and execute it with the function call arguments (*result.tool_input*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eecdb307-8470-494c-a568-6fd52136b1e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.schema.agent import AgentFinish\n",
    "\n",
    "def route(result):\n",
    "    if isinstance(result, AgentFinish):\n",
    "        return result.return_values['output']\n",
    "    else:\n",
    "        tools = {\n",
    "            \"get_current_temperature\": get_current_temperature,\n",
    "            \"get_country_information\": get_country_information,\n",
    "        }\n",
    "        print(result.tool_input) # When using a tool, let's print the input / arguments of the tool\n",
    "        print(result.tool) \n",
    "        print(tools[result.tool].run(result.tool_input))\n",
    "        return tools[result.tool].run(result.tool_input) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a928b8f9-506a-4347-99a0-9ba7c7733c46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain = prompt | h2ogpteChat_bind | OpenAIFunctionsAgentOutputParser() | route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dd680a6b-8ce2-409d-9c75-a7075901f339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'country': 'Spain', 'field_to_extract': 'maps'}\n",
      "get_country_information\n",
      "Spain's maps is {'googleMaps': 'https://goo.gl/maps/138JaXW8EZzRVitY9', 'openStreetMaps': 'https://www.openstreetmap.org/relation/1311341'}\n"
     ]
    }
   ],
   "source": [
    "result = chain.invoke({\"input\": \"Give me map of Spain?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7c2052f5-2af7-4743-9fc3-7a3e62e70036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Spain's maps is {'googleMaps': 'https://goo.gl/maps/138JaXW8EZzRVitY9', 'openStreetMaps': 'https://www.openstreetmap.org/relation/1311341'}\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "844995e2-60d5-41ca-a6ce-4ca4545fec65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'latitude': 37.7749, 'longitude': -122.4194}\n",
      "get_current_temperature\n",
      "The current temperature is 18.9°C\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The current temperature is 18.9°C'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = chain.invoke({\"input\": \"hello, what is the temperature in San Francisco, CA?\"}) \n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1b677f1a-3046-43bb-8ba4-2232c6ae57ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'country': 'Hungary', 'field_to_extract': 'population'}\n",
      "get_country_information\n",
      "Hungary's population is 9749763\n"
     ]
    }
   ],
   "source": [
    "result = chain.invoke({\"input\": \"What is the number of inhabitants in Hungary?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d60dbcf5-2d11-403d-8df3-0805d50f826c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hungary's population is 9749763\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3b5e0d-b51e-465a-91f5-a5d98a3fe569",
   "metadata": {},
   "source": [
    "<!-- ## TODO WORD ON GUARDRAILS -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18890e9-77ff-43a8-bb12-02a3d42a0842",
   "metadata": {},
   "source": [
    "We are almost there! \n",
    "\n",
    "Let's add some messages placeholder, that is a placeholder for intermediary steps or scratchpad that should be a sequence of messages that contains the previous agent tool invocations and the corresponding tool outputs put together. This will be useful to further enhance our agent in using previous messages, including initial user query and the intermediary steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577610f5-9a48-43ba-b43c-11b937927484",
   "metadata": {},
   "source": [
    "<!-- ## TODO MessagesPlaceholder explanation -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e29f0f4c-892b-4f44-bed1-c6c0643673ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts.chat import HumanMessagePromptTemplate\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    HumanMessagePromptTemplate.from_template(\"My name is Audrey, {input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"intermediate_steps\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6c9837-13dd-4573-ac2a-fa03a3482f2f",
   "metadata": {},
   "source": [
    "First, we will be borrowing the [format_to_openai_function_messages](https://api.python.langchain.com/en/latest/_modules/langchain/agents/format_scratchpad/openai_functions.html#format_to_openai_function_messages), which simply \n",
    "send to the LLM the Steps the LLM has taken to date, along with observations and returns a list of messages to send to the LLM for the next prediction. For this we also sligtly modified the h2ogpteChat Model class to check if the initial query has been answered or not yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "16fc461e-734e-420f-b31c-81aa93de3a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.format_scratchpad.openai_functions import format_to_openai_function_messages\n",
    "from langchain.schema.runnable import RunnablePassthrough \n",
    "\n",
    "\n",
    "agent_chain = RunnablePassthrough.assign(\n",
    "    intermediate_steps= lambda x: format_to_openai_function_messages(x[\"intermediate_steps\"])\n",
    ") | prompt | h2ogpteChat_bind | OpenAIFunctionsAgentOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "92f76237-e6ea-417b-984b-81a932784047",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = AgentExecutor(agent=agent_chain, \n",
    "                               tools=[get_country_information, get_current_temperature], \n",
    "                               verbose=True, \n",
    "                               handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0363a2e8-7b32-490d-9d19-1be7d37f2f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_current_temperature` with `{'latitude': '43.5333', 'longitude': '5.0833'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mThe current temperature is 20.6°C\u001b[0m\u001b[32;1m\u001b[1;3mThe current temperature is 20.6°C\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is the current temperature in Istres, France in degrees celcius?',\n",
       " 'output': 'The current temperature is 20.6°C'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"what is the current temperature in Istres, France in degrees celcius?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ae00036e-aa41-4cca-a64d-830888fedc6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_country_information` with `{'country': 'Hungary', 'field_to_extract': 'maps'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mHungary's maps is {'googleMaps': 'https://goo.gl/maps/9gfPupm5bffixiFJ6', 'openStreetMaps': 'https://www.openstreetmap.org/relation/21335'}\u001b[0m\u001b[32;1m\u001b[1;3mHungary's maps is {'googleMaps': 'https://goo.gl/maps/9gfPupm5bffixiFJ6', 'openStreetMaps': 'https://www.openstreetmap.org/relation/21335'}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Give me a map of Hungary?',\n",
       " 'output': \"Hungary's maps is {'googleMaps': 'https://goo.gl/maps/9gfPupm5bffixiFJ6', 'openStreetMaps': 'https://www.openstreetmap.org/relation/21335'}\"}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"Give me a map of Hungary?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6ba8a008-1222-4883-8d6e-e323a8c3981f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mHello Audrey, in simplified Chinese your name would be 奥德里, pronounced as \"Aòdélǐ\". Please note that Chinese names can have many variations and this is one of them.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is my name in chinese',\n",
       " 'output': 'Hello Audrey, in simplified Chinese your name would be 奥德里, pronounced as \"Aòdélǐ\". Please note that Chinese names can have many variations and this is one of them.'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"what is my name in chinese\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2f1195-560b-4932-8ee6-26ae584c7adf",
   "metadata": {},
   "source": [
    "##\n",
    "\n",
    "Yay! we created an agent wiht h2ogpte and Langchain!\n",
    "\n",
    "##\n",
    "\n",
    "Now, obviously this is a very simple agent and if you wanted go a step further, you can try and improve:\n",
    "\n",
    "- error handling: for example if the function fails due to unavailability or invalid arguments\n",
    "- Handling simulatenous or sequential function call: Modify the h2ogpteChat model to handle intermediary steps that require multiple function call for more complex user query\n",
    "- Handling Memory: enabling the agent to keep a chat history for follow up questions\n",
    "- Prompt engineering: test out different Instructions or Few-shot learning examples to enforce a model behavior through all conversations and messages\n",
    "- Guardrails: enable h2ogpte guardrails and create your own! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a15c0e5-9769-4715-bec3-7e4cfcd54411",
   "metadata": {},
   "source": [
    "##\n",
    "##\n",
    "#### Reference sources: h2oGPTe, Langchain, DeepLearning.ai, Medium Blog ressources\n",
    "\n",
    "\n",
    "- https://h2ogpte.genai.h2o.ai/\n",
    "- https://docs.h2o.ai/enterprise-h2ogpte/\n",
    "- https://docs.h2o.ai/enterprise-h2ogpte/get-started/use-cases\n",
    "\n",
    "- https://blog.spheron.network/a-comprehensive-comparison-of-llm-chaining-frameworks\n",
    "- https://python.langchain.com/v0.2/docs/how_to/custom_chat_model/\n",
    "- https://learn.deeplearning.ai/courses/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
